{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s223171213/Documents/machine-learning-docs/.mld-venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('malware', axis=1)\n",
    "y = data['malware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(100,), activation=activation_function))\n",
    "    for _ in range(9):\n",
    "        model.add(Dense(64, activation=activation_function))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"Training model with {activation} activation function...\")\n",
    "    model = build_model(activation)\n",
    "    \n",
    "    history = model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping], verbose=1, batch_size=128)\n",
    "    \n",
    "    y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[activation] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"Finished training with {activation} activation function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         accuracy  precision    recall  f1_score\n",
      "relu     0.984959   0.985131  0.999649  0.992337\n",
      "sigmoid  0.974248   0.974248  1.000000  0.986956\n",
      "tanh     0.984389   0.986582  0.997544  0.992033\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of a Multi-Layer Perceptron (MLP) model with 10 hidden layers, I tested three activation functions: ReLU, sigmoid, and tanh, using the Adam solver on the \"data.csv\" dataset. The model's performance metrics included accuracy, precision, recall, and F1 score. ReLU yielded the best performance with an accuracy of 0.984959, precision of 0.985131, recall of 0.999649, and an F1 score of 0.992337. Tanh was slightly behind, while sigmoid performed the least effectively. ReLU's superior performance is attributed to its ability to mitigate the vanishing gradient problem, facilitating deeper network training and faster convergence. These results suggest that ReLU is the most effective activation function for this MLP configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mld-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
