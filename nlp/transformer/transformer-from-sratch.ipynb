{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronnguyen/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996,  3185,  2001, 10392,   999]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"The movie was fantastic!\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print(inputs.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronnguyen/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Load model configuration\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the embedding layer\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "# Generate embeddings for the input tokens\n",
    "input_embs = token_emb(inputs.input_ids)\n",
    "print(input_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "query = key = value = input_embs\n",
    "weighted_value = scaled_dot_product_attention(query, key, value)\n",
    "print(weighted_value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(input_embs)\n",
    "print(attn_output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(input_embs)\n",
    "print(attn_output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)\n",
    "print(ff_outputs.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        x = x + self.attention(hidden_state)\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "print(encoder_layer(input_embs).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "embedding_layer = Embeddings(config)\n",
    "print(embedding_layer(inputs.input_ids).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = TransformerEncoder(config)\n",
    "print(encoder(inputs.input_ids).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "config.num_labels = 2\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "print(encoder_classifier(inputs.input_ids).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1996  3185  2001 10392   999]], shape=(1, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_for_sequence_classification_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_for_sequence_classification_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformer_encoder_1           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_937 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ transformer_encoder_1           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_937 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerForSequenceClassification.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerForSequenceClassification.call():\n  • input_ids=tf.Tensor(shape=(1, 5), dtype=int32)\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Print the outputs shape\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[12], line 130\u001b[0m, in \u001b[0;36mTransformerForSequenceClassification.call\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x[:, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TransformerForSequenceClassification.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerForSequenceClassification.call():\n  • input_ids=tf.Tensor(shape=(1, 5), dtype=int32)\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"The movie was fantastic!\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"tf\", add_special_tokens=False)\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "# Load model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the embedding layer\n",
    "class Embeddings(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = layers.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = layers.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        seq_length = tf.shape(input_ids)[1]\n",
    "        position_ids = tf.range(seq_length)[tf.newaxis, :]\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, value)\n",
    "\n",
    "# Attention Head\n",
    "class AttentionHead(layers.Layer):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = layers.Dense(head_dim)\n",
    "        self.k = layers.Dense(head_dim)\n",
    "        self.v = layers.Dense(head_dim)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        query = self.q(hidden_state)\n",
    "        key = self.k(hidden_state)\n",
    "        value = self.v(hidden_state)\n",
    "        attn_output = scaled_dot_product_attention(query, key, value, mask)\n",
    "        return attn_output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.attention_heads = [AttentionHead(config.hidden_size, self.head_dim) for _ in range(self.num_heads)]\n",
    "        self.output_linear = layers.Dense(config.hidden_size)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        attn_outputs = [head(hidden_state, mask) for head in self.attention_heads]\n",
    "        concat_attn = tf.concat(attn_outputs, axis=-1)\n",
    "        output = self.output_linear(concat_attn)\n",
    "        return output\n",
    "\n",
    "# Feed Forward Layer\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = layers.Dense(config.intermediate_size, activation='gelu')\n",
    "        self.linear_2 = layers.Dense(config.hidden_size)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.layer_norm_2 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attention_output = self.attention(hidden_state, mask)\n",
    "        x = x + attention_output\n",
    "        feed_forward_output = self.feed_forward(self.layer_norm_2(x))\n",
    "        x = x + feed_forward_output\n",
    "        return x\n",
    "\n",
    "# Full Transformer Encoder\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_layers = [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(self, input_ids, mask=None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# Sequence Classification Model\n",
    "class TransformerForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = layers.Dense(config.num_labels)\n",
    "\n",
    "    def call(self, input_ids, mask=None):\n",
    "        x = self.encoder(input_ids, mask)\n",
    "        x = self.dropout(x[:, 0, :])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronnguyen/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the IMDB dataset\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(texts, tokenizer, max_len=128):\n",
    "    tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_len) for text in texts]\n",
    "    padded_texts = pad_sequences(tokenized_texts, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded_texts\n",
    "\n",
    "# Convert IMDB dataset to text\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {index + 3: word for word, index in word_index.items()}\n",
    "index_word[0] = '[PAD]'\n",
    "index_word[1] = '[START]'\n",
    "index_word[2] = '[UNK]'\n",
    "index_word[3] = '[UNUSED]'\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([index_word.get(i, '?') for i in encoded_review])\n",
    "\n",
    "x_train_texts = [decode_review(x) for x in x_train]\n",
    "x_test_texts = [decode_review(x) for x in x_test]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 512\n",
    "x_train_padded = tokenize_and_pad(x_train_texts, tokenizer, max_len)\n",
    "x_test_padded = tokenize_and_pad(x_test_texts, tokenizer, max_len)\n",
    "\n",
    "# Load model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_labels = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding layer\n",
    "class Embeddings(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = layers.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = layers.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        seq_length = tf.shape(input_ids)[1]\n",
    "        position_ids = tf.range(seq_length)[tf.newaxis, :]\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, value)\n",
    "\n",
    "# Attention Head\n",
    "class AttentionHead(layers.Layer):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = layers.Dense(head_dim)\n",
    "        self.k = layers.Dense(head_dim)\n",
    "        self.v = layers.Dense(head_dim)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        query = self.q(hidden_state)\n",
    "        key = self.k(hidden_state)\n",
    "        value = self.v(hidden_state)\n",
    "        attn_output = scaled_dot_product_attention(query, key, value, mask)\n",
    "        return attn_output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.attention_heads = [AttentionHead(config.hidden_size, self.head_dim) for _ in range(self.num_heads)]\n",
    "        self.output_linear = layers.Dense(config.hidden_size)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        attn_outputs = [head(hidden_state, mask) for head in self.attention_heads]\n",
    "        concat_attn = tf.concat(attn_outputs, axis=-1)\n",
    "        output = self.output_linear(concat_attn)\n",
    "        return output\n",
    "\n",
    "# Feed Forward Layer\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = layers.Dense(config.intermediate_size, activation='gelu')\n",
    "        self.linear_2 = layers.Dense(config.hidden_size)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.layer_norm_2 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attention_output = self.attention(hidden_state, mask)\n",
    "        x = x + attention_output\n",
    "        feed_forward_output = self.feed_forward(self.layer_norm_2(x))\n",
    "        x = x + feed_forward_output\n",
    "        return x\n",
    "\n",
    "# Full Transformer Encoder\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_layers = [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(self, input_ids, mask=None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Classification Model\n",
    "class TransformerForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = layers.Dense(config.num_labels)\n",
    "\n",
    "    def call(self, input_ids, mask=None):\n",
    "        x = self.encoder(input_ids, mask)\n",
    "        x = self.dropout(x[:, 0, :])\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = TransformerForSequenceClassification(config)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1031, 2707, ...,    0,    0,    0],\n",
       "       [ 101, 1031, 2707, ...,    0,    0,    0],\n",
       "       [ 101, 1031, 2707, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 1031, 2707, ...,    0,    0,    0],\n",
       "       [ 101, 1031, 2707, ...,    0,    0,    0],\n",
       "       [ 101, 1031, 2707, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronnguyen/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/keras/src/layers/layer.py:1295: UserWarning: Layer 'transformer_for_sequence_classification_4' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)''\n",
      "  warnings.warn(\n",
      "/Users/aaronnguyen/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'transformer_for_sequence_classification_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerForSequenceClassification.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerForSequenceClassification.call():\n  • input_ids={'input_ids': 'tf.Tensor(shape=(None, 512), dtype=int32)'}\n  • mask={'input_ids': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mTransformerForSequenceClassification.call\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x[:, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TransformerForSequenceClassification.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerForSequenceClassification.call():\n  • input_ids={'input_ids': 'tf.Tensor(shape=(None, 512), dtype=int32)'}\n  • mask={'input_ids': 'None'}"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, y_train, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(x_test_padded)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • input_ids=tf.Tensor(shape=(None, 512), dtype=int32)\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 195\u001b[0m\n\u001b[1;32m    192\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    198\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_data)\n",
      "File \u001b[0;32m~/Documents/side-projects/machine-learning-docs/.mld-venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[34], line 177\u001b[0m, in \u001b[0;36mTransformerForSequenceClassification.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    176\u001b[0m     input_ids, mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 177\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x[:, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[1;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "Cell \u001b[0;32mIn[34], line 161\u001b[0m, in \u001b[0;36mTransformerEncoder.call\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m--> 161\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: None (of type <class 'NoneType'>)\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • input_ids=tf.Tensor(shape=(None, 512), dtype=int32)\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the IMDB dataset\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(texts, tokenizer, max_len=128):\n",
    "    tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_len) for text in texts]\n",
    "    padded_texts = pad_sequences(tokenized_texts, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded_texts\n",
    "\n",
    "# Convert IMDB dataset to text\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {index + 3: word for word, index in word_index.items()}\n",
    "index_word[0] = '[PAD]'\n",
    "index_word[1] = '[START]'\n",
    "index_word[2] = '[UNK]'\n",
    "index_word[3] = '[UNUSED]'\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([index_word.get(i, '?') for i in encoded_review])\n",
    "\n",
    "x_train_texts = [decode_review(x) for x in x_train]\n",
    "x_test_texts = [decode_review(x) for x in x_test]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 512\n",
    "x_train_padded = tokenize_and_pad(x_train_texts, tokenizer, max_len)\n",
    "x_test_padded = tokenize_and_pad(x_test_texts, tokenizer, max_len)\n",
    "\n",
    "# Load model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_labels = 2\n",
    "\n",
    "# Define the embedding layer\n",
    "class Embeddings(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = layers.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = layers.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        seq_length = tf.shape(input_ids)[1]\n",
    "        position_ids = tf.range(seq_length)[tf.newaxis, :]\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, value)\n",
    "\n",
    "# Attention Head\n",
    "class AttentionHead(layers.Layer):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = layers.Dense(head_dim)\n",
    "        self.k = layers.Dense(head_dim)\n",
    "        self.v = layers.Dense(head_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        query = self.q(hidden_state)\n",
    "        key = self.k(hidden_state)\n",
    "        value = self.v(hidden_state)\n",
    "        attn_output = scaled_dot_product_attention(query, key, value, mask)\n",
    "        return attn_output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.attention_heads = [AttentionHead(config.hidden_size, self.head_dim) for _ in range(self.num_heads)]\n",
    "        self.output_linear = layers.Dense(config.hidden_size)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_state, mask=None):\n",
    "        attn_outputs = [head(hidden_state, mask) for head in self.attention_heads]\n",
    "        concat_attn = tf.concat(attn_outputs, axis=-1)\n",
    "        output = self.output_linear(concat_attn)\n",
    "        return output\n",
    "\n",
    "# Feed Forward Layer\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = layers.Dense(config.intermediate_size, activation='gelu')\n",
    "        self.linear_2 = layers.Dense(config.hidden_size)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.layer_norm_2 = layers.LayerNormalization(epsilon=1e-12)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attention_output = self.attention(hidden_state, mask)\n",
    "        x = x + attention_output\n",
    "        feed_forward_output = self.feed_forward(self.layer_norm_2(x))\n",
    "        x = x + feed_forward_output\n",
    "        return x\n",
    "\n",
    "# Full Transformer Encoder\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_layers = [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input_ids, mask=None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# Sequence Classification Model\n",
    "class TransformerForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = layers.Dense(config.num_labels)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, mask = inputs['input_ids'], inputs.get('mask', None)\n",
    "        x = self.encoder(input_ids=input_ids, mask=mask)\n",
    "        x = self.dropout(x[:, 0, :])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Prepare the data for the model\n",
    "train_data = {'input_ids': tf.convert_to_tensor(x_train_padded)}\n",
    "test_data = {'input_ids': tf.convert_to_tensor(x_test_padded)}\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerForSequenceClassification(config)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, y_train, batch_size=32, epochs=3, validation_data=(test_data, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mld-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
